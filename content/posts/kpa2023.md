+++
title = 'Kpa2023'
date = 2023-10-19T16:15:12+01:00
draft = false
+++

# 2023 Belfast Kainos Platform Academy

###  My Background

First up, I'll introduce myself a little to give you an idea of the type of people coming into the Academy. That said, I was impressed with the variety of backgrounds we all came from, each bringing different experiences allowing each of us to learn a lot from one another. 

My name is Patrick and I've worked in the IT industry for coming on 7 years now.

I started out in Tech as *'the IT guy'* at a local factory and worked my way through several jobs until my previous role at Ivanti where I worked as a TSE 2 for their product 'Security Controls' - a self hosted patch management solution designed for SMEs to large orgs in and around the few thousand employees mark. That was my first foray at the vendor side of the equation and I advanced a lot throughout my time there, building skills beyond the traditional desk-side support type roles I'd previously held.

This gave me significantly more satisfaction in work but I was still missing something. I'd always been a tinkerer, playing around with Raspberry Pis, running Linux on my home PC (I've seen too much of Windows in my day jobs to run it at home as well) as well as some hobbyist electronics. 

> I am a classic nerd really, in fact that's how I ended up in IT in the first place. Initially, I'd not had a great time while at Uni so I dropped out, twice actually! Later, I got my first IT job simply for the (nerdy) answers I gave in an interview for a different role, so I very much *'fell into IT'.*


Outside of work however, I'd always been interested in generally more technical things than turning computers off and on for PICNIC users. Even in my most recent role, supporting *actual IT admins*, I often found the problems I was working on to still be in that same category! I needed something more. The last two years in particular I'd been gaining steady momentum learning Python in my free time for nothing more than fun and exploring other avenues such as data analysis, ML etc. I even wrote my own central heating controller system in Python using a couple of Raspberry Pis, temperature sensors and a relay board. (This was in part because I lost the old control unit when renovating and thought they cost more than they were worth when looking to buy a new one).

So, come June 2023, I knew deep down that I was ultimately missing something from my work life, but I wasn't yet actively looking. Perhaps I waited longer than I needed to make the jump (I'm sure we're all familiar with **Imposter Syndrome**) but I like to think of it as very good timing that I saw a particular post on LinkedIn seemingly answering my desire for change.

The post itself was very casual and honestly looked too good to be true. Something along the lines of 'IT people who are interested in DevOps, looking to make a career transition' and '8 weeks fully paid training in our Academy'. OK, this looks interesting. I didn't see a catch, I was *sure there was one* but thought what's the harm in reaching out? (I've still not found the catch!)


> Fast forward to now and the 8 weeks of the Academy are still settling in my mind. A fantastic, intense, fast, long, diverse, engaging, challenging and fun 8 weeks.


This article is partly an overview of my experiences of the Academy and partly an appreciation post for the opportunity Kainos has afforded myself and the 7 other new joiners in the Academy, as well as some nerdy tangents along the way.


### 'The 8 weeks'

Let's first break down the format of the 8 weeks then.

The Academy was taken by Senior Platform Engineer John Boyle, who throughout exhibited the Kainos culture exceptionally, along with boundless patience! He'd taken content from previous iterations of the Academy and modified them to be slightly more up date, for example, the curriculum now included more containerisation than before.

> John would later tell us that some of the content such as Kubernetes was also new to him and so he had to keep one step ahead of us outside of work hours. He really put a huge amount of time and effort into providing high value content and he deserves huge gratitude not just from us but all of Platforms and Kainos as a whole for his efforts.


| Week | Overview           | Description                                                                                                         |
| ---- | ------------------ | ------------------------------------------------------------------------------------------------------------------- |
| 1    | Know your tools    | Installing and configuring tooling for the rest of the course. Some introduction to cloud platforms.                |
| 2    | Azure              | Two days theory on Azure platform then practical tasks to cement learning.                                          |
| 3    | AWS                | Two days theory on AWS platform then practical tasks to cement learning.                                            |
| 4    | Advanced Terraform | Using AWS, continuing on from examples in previous weeks using more advanced TF features such as modules and loops, greater use of variables, remote state etc. |
| 5    | Guest Talks        | From CI/CD to cloud Security, a wide range of guest talks on numerous topics.                                                                                                |
| 6    | Containers         | Docker and Kubernetes lectures and practical exercises.                                                            |
| 7    | Sock Shop          | Final Project to deploy a micro-services based website which sells socks.                                                                                                                    |
| 8    | Sock Shop          | Daily stand-ups with the 'customer' (Brad) and a final presentation at the end of the two weeks.                                                                                                                    |


### Week 0

In the beginning... we had a lot of meetings and introduced ourselves many, many times...

My name is Patrick, I've got 3 dogs and 3 cats, I used to do X, I'm here to do Y...

Joking aside, I did really appreciate the efficiency and atmosphere of the initial company onboarding sessions. While still a *lot of meetings* to attend and a lot of corporate information to absorb, it was streamlined compared to numerous other companies I've worked for and the **culture genuinely came across well**. I especially appreciated that Brendan attended the first onboarding meeting to welcome everyone, you don't see that sort of thing in a lot of companies of this size!

---

### Week 1

Now it was time to get stuck into the Academy proper. We all had our... looks at notes, yes that's correct; Macbook Pros, and were eager and ready to get stuck in. 'Hang on, we use Macs, here? I've never used a Mac before, have you?' You'd hear one of us say. 'Nope, only Windows', 'I've used Linux before but not a computer as shiny as this...' others would answer. As chance would have it, none of us were experienced in the way of the shiny fruit, so that was our first stumbling block. That said, I've personally become quite fond of my Kainos laptop and it's getting harder every day switching between it and my Linux desktop, the muscle memory is beginning to favour the Apple way now! (Don't tell my many past IT managers) However, another stumbling block which was to rear its head on several occasions throughout the Academy was the controversial **Apple Silicone**. Combined with the dreaded Cyberark and Zscaler, we certainly came up against some difficult software installation scenarios!


![Linux, Mac, Windows meme](/blog/kpa_img/Windows_Vs_Mac_Vs_Linux_10.jpg)


Once we'd managed to get over the first humps of software installations (there would be more in future weeks) it was time to get stuck in to the actual job of learning.

So, what are we here to actually do?

Let's start with some simple ClickOps tasks to hammer home the main point of IaC. Deploy an Ubuntu web server and mess about with users, groups, partitions and stuff like that. Good, honest, mundane admin tasks.


> After this, do it again, but in this new magical language called **Terraform**. 


Well that's definitely better! [^1]

OK, I think we all get it now, IaC is the way to go, ain't nobody got time for all that clicking. Lesson 1 - complete!

The rest of the week mixed in some further simple TF deployment tasks with some theory items such as 'Introduction to Cloud Security'. All in all, a good start to the Academy, nothing too scary and I think we all felt fairly confident at this point. Roll on week 2!


---

### Week 2

Now that we were all set up for the most part and on a good base, it was time to dig into some more depth into the cloud platforms predominantly used. First up, **Azure**.

Two days of predominantly theory in the morning, followed by practical tasks in the afternoons allowed us to all build a solid baseline understanding of how to work with Azure. 

Then, for the last 3 days of the week, a larger exercise deploying a web-app using App Service with a Private endpoint to a PG DB, private DNS and surrounding infrastructure. All deployed using basic TF.


![azure diagram](/blog/kpa_img/azure_diagram.png)

---

### Week 3

Similar to week 2 but this time exploring the AWS side and using slightly more advanced architecture, we had two days of mixed theory and practical tasks followed by three days deploying another web app. This time using containers, integrating a public domain name and SSL certificate as well as using a load a balancer. With limited time and technical hiccups however, getting the full deployment with ECS was a challenge too far, so we opted simply to go with load balanced VMs running the application in order to cover all of the other areas of the infrastructure. We had to be agile of course!

![aws diagram](/blog/kpa_img/aws_diagram.png)

> So by the end of Weeks 3 and 4, apart from missing out Fargate, we were pretty much all now cloud experts and ready for the big time.

---

### Week 4

Advanced Terraform time. Now we were starting to pull up and down infrastructure like nobody's business.

![terraform meme](/blog/kpa_img/tfmeme2.jpeg)

But some of the stuff we were building wasn't exactly speedy to bring up or down...

![Terraform meme](/blog/kpa_img/tfmeme.png)

So we were taught techniques such as breaking up the infrastructure codebase into components and using modules to facilitate more segmented deployments and tear-downs. We were also encouraged to pull as much as possible into variables and locals among some other such best practices. 


![](/blog/kpa_img/tree.png) A fairly simple demonstration task from week 4, already getting pretty unruly and large!


> Perhaps most usefully however, the simple lifecycle argument of `create_before_destroy`  saved us the most time overall.


Of course, there were at several times throughout the week fleeting moments of confusion, as for most of us, Terraform was in itself a new language and some of the more advanced techniques and methodologies were a little abstract to what some were used to. In particular, the logic of how variables and outputs flow up and down through module instantiations in segregated component folders caused confusion for some. This 3 layer, bi-directional waterfall model took a little time to wrap all of our heads round. 

![picture of hand written note about terraform variables](/blog/kpa_img/tfvars.jpg)


Further, we found that examples online for TF are particularly susceptible to premature aging. Even 6 months in TF and cloud infrastructure terms can see such huge breaking changes that SEO simply can't keep up and LLMs act as though trained on ancient manuscripts from the annals of IaC history. You find yourself perusing through countless tutorials and guides based on previous versions of both TF and providers; to the effect that none of what they show actually builds and produces obscure error messaging setting you off on further quests down search engine rabbit holes. Coupled with the official TF registry docs making **hard assumptions** about your existing knowledge of cloud components, getting _good_ example based guidance to deploy _current_ versions of cloud infrastructure was often a real challenge. This in and of itself was of course a major lesson. 

> **Cloud moves fast!**


Beyond that however, I feel the TF ecosystem lacks a certain rigour of standardisations of use and best practices, than other more mature and traditional languages do. Yes there's a certain amount of linting available with the `fmt` subcommand and tooling exists to highlight issues within the context of the infrastructure being deployed, but I found little consensus around simple things such as project structure. Perhaps I am naive and spoiled by modern Python, where I do feel structural standards are well established (although not followed by all), and my limited experience of professional software development definitely limits the value of my opinion on this point, but it is something which stood out to me while learning the language.

> By the end of this week, we would be seeing Terraform error messages when we closed our eyes to go to sleep at night.

### Week 5

With John out for a well deserved holiday, we had a bit of a break in format with several diverse guest lectures filling in the time and some self paced learning. This to me was by on large a very interesting week.

It started with CI/CD taken by Jamie Hopper for the first 2 days.

Jamie had created a dummy project complete with init and compose scripts as well as pipelines.  A sort of 'gold standard' baseline for a good CI/CD implementation. Though masterfully designed, he too was caught off guard by our cult of M2 wielding 'appleites'. 

> 'It worked on his machine!'

He'd created some simple Go based web API as the 'application' to deploy, it was dockerised and there were tasks of increasing difficulty in the GitHub issues for us to get stuck into after some initial theory lectures on CI/CD.

> I had never used the Go language before, but I was willing to, well, you can see the pun writing itself there...

First we had to overcome our installation issues with M2 and Cyberark. Jamie did some magic in the background editing his init scripts to cope with the hobbled laptop architectures and after we all raised IT requests to get certain dependencies in place (which required root permissions), we were ready to go. It made a perfect example of the lesson we'd just had about decoupling the pipeline logic from the runner platform and handling differences in local and remote environments within your own CI/CD scripting!


After our exploration of CI/CD, we had numerous smaller sessions with different people on a range of subjects.

A big shout out to everyone who took the time to speak with us during the Academy and to share their experiences and expertise.

- Experiences from the Academy and Beyond - Jake Moorhead
- Azure Machine Learning at Royal Sun Alliance (RSA) - Tony Skidmore
- CICD - Jamie Hopper
- Managed Services - Matthew Craig, Cara Leitch, Warren Zingel, Donna Hull
- Well Architected - Jack Morris, Radomir Mocarski, Michael Saparov, Bhups Hirani
- Security - Ghulam Abbas
- Generative A.I - Thomas Thornton
- Kubernetes - Rees Pozzi
- Scrum and Kanban - Vishal Chavda

### Week 6

In a way, this was the last week of the Academy in terms of the curriculum, with the following weeks being dedicated to the final project, pulling together everything we'd learned.

So, to keep it light, we focused mainly on Kubernetes this week.

![kubernetes meme](/blog/kpa_img/k8smeme.jpg)

First, we started with a bit of Docker and general containerisation theory. Again, the M2 Macs put some roadblocks in the way here - I've run out of silly puns to address this however. I'd dabbled with docker a little in my previous life, mainly in hobbyist projects though and not in a professional capacity. For most however, this was a new technology, and as can be imagined, a fairly challenging one. To get to grips with it, we followed along with the official docker 101 tutorial and later a more advanced WP & MySql challenge. Again, because of the Macs we had to run this in the cloud but that was no problem as we are Platform Engineers![^2]

> Once we all had some basic containers running though, it was time for the _pièce de résistance_: Kubernetes. I should give a shout out to Nana here - not a Kainos alumni to my knowledge but her videos on subjects such as this were invaluable throughout the Academy and beyond!

Again, Minikube wouldn't work straight away on our Macs and so rather than waste time trying we went again straight to the cloud. (I've since installed Microk8s on my home desktop PC to play around further with k8s without incurring the wrath of cloud billing demons)

We used Thomas Thornton's [DevOps The Hard Way in Azure](https://thomasthornton.cloud/2021/10/25/devops-the-hard-way-in-azure/) post to get to grips with the basics of k8s - another big shout out to him! This definitely gave us a lot of necessary know-how for our upcoming final project in weeks 7 and 8.



### Weeks 7 and 8

I believe at this point, some previous incarnations of the Academy had already finished. For us, there were two more weeks to go but these two weeks had a distinctly different feel to them. This was to give us a baptism of fire so-to-speak into what project work was going to be like.

The task: deploy a 'Sock-Shop' in the cloud using Kubernetes and all of the required infrastructure to support it, using Azure DevOps. The application itself was already written by our imaginary devs (The team at Weaveworks from 6/7 years ago) which demonstrated a typical (if a little old) MicroServices application. We were split into 3 teams, 2 using AWS and my team running with Azure.

The basic demo provided by Weaveworks as is, consists of the following containers:

- mongo (image used for some of the dbs)
- redis
- rabbitmq
- carts
- catalogue
- catalogue-db (a mysql db)
- front-end
- orders
- payment
- queue-master
- shipping
- user
- user-db (another mongo db but with some customisations from the official mongo image)

When deployed fully inside Kubernetes, the site works as you would expect and includes some lovely pairs of socks!

![sock shop website demo screenshot](/blog/kpa_img/sock-shop.png)

Our task was to build this out into more production ready infrastructure in the cloud. We had a few project requirements and several nice-to-haves as well as daily stand-ups with our customer - Brad Taylor. For example, it was desired to have managed DBs in the cloud platform rather than containerised ones.

We'd need to use everything we'd learned over the previous 6 weeks to accomplish this task. It would also teach us many other lessons and soft skills such as teamwork, communication and dealing with merge conflicts... peacefully!

My team's repo by the end of the two weeks would end up looking like this:

![](/blog/kpa_img/tree2.png)


I don't have access to the DevOps board anymore of course and unfortunately neglected to take screenshots at the time. However, overall, our project, along with the other teams were all considered a success. While there would certainly have been room for improvements, all of the core requirements were met with each team displaying a good range of additional 'nice-to-haves'. For 'one sprint', we were all more than happy with what we'd achieved. Further, we all had a good understanding of what we would do in a second sprint, were there to be one. We had learned a great deal about estimating the time involved to complete the items of work and that's not something you can really teach without doing!

One personal highlight that comes to mind from the project was getting SSL working for our team. As the public domain owned by Kainos for the KPA was registered in AWS, but my team was deploying into Azure, we needed to install a certificate in Azure that was referenced by AWS DNS. Now that didn't sound too difficult to me on the offset, I'd some prior experience with certs from previous IT roles. This turned into a bit of a lesson in **vendor lock-in**.

To get a certificate to be imported into Azure, as with any service really, you obviously need the public **and private** components. While creating an SSL certificate within AWS (where the DNS needed to be), doing so did not allow the export of the private key... OK, let's look at Azure then, we just need the DNS in AWS to point to our gateway in Azure anyway, it'll be fine.

Well, that would have been nice, but to create a certificate within Azure, it must be associated with a limited list of Azure services if it is to be signed by Microsoft CAs. Crucially, we could not create one with DNS validation without the DNS existing also in Azure. The more generic Generate/Import certificate feature within KeyVault looked promising but for it not to be self signed required the same steps of completing the certificate request as if you were to have it made outside of Azure were required. Thus creating it inside Azure simply made the process more clunky and so an externally created certificate was the way to go.

Enter the freeloaders solution - Let's Encrypt!

![let's encrypt advert](/blog/kpa_img/letsencrypt.jpeg)

I knew Let's Encrypt provided free SSL certificates as part of the old movement to secure the web (remember when green padlocks were rare). So I went about creating an SSL certificate manually as per their instructions. This was not too difficult really but it was hard enough finding a guide which outlined the manual process as opposed to installing the `certbot` tool on your web-server directly to perform auto-renewal type situations, which was not appropriate for our scenario. You need to run the `certbot` utility with specific flags to force it to run manually and generate the certificate only, and we needed to specify DNS validation instead of the default. This involved simply adding a TXT record in the DNS to prove ownership of the site. Now we could import the certificate into Azure.


---


One of the few regrets I had, personally, was not getting the databases actually integrated into the application, but it seems this may have been more to do with the legacy codebase than anything truly platform related. The base demo used databases in containers, and while the infrastructure for the managed databases (both MySQL and Mongo) were all complete in our deployments, and at least in my team's case (I only know about my team), tested as working, they were not being *used* by the actual application. 

The main stumbling block for this seemed to be some incompatibilities between defaults used in the rather old version of the databases used by the Weaveworks demo, the current versions available on cloud platforms and the legacy code itself. I spent a fairly large amount of time after hours running down further rabbit holes here trying to get this working, and although ultimately unfruitful, I learned a lot along the way.

The catalogue service for example, an API written in `'go'`, connects to the catalogue-db service in the purely k8s demo. I reverse engineered the code to figure out how to pass the connection string for our database in Azure, this was simple enough, the code included optional flags which could be passed to the container.

I tested the connection to the database from within the Azure VPC - to do this I needed to deploy a management VM and install MySQL and some dependencies - not a problem at all with my new TF prowess of course! To my pleasant surprise I could connect to the db with no issue. I then used this VM to create the db, tables and run the initialising SQL script with the catalogue data.

Still, the catalogue service would not successfully connect to our DB.

I thought, perhaps there is a networking issue between our K8s cluster and the DB, after all, this was all new to all of us still. 

![image of terminal commands running mysql queries](/blog/kpa_img/mysql_query.png)
This was quickly ruled out. 

So I knew the database was working fine, had the tables created and data inserted and could be accessed from a container running on the cluster. I even wrote a simple go connection test and ran it from the management VM, though using the *latest* version of go. What could possibly be the problem then?

After the Academy, I came back to the issue to try and further understand it (letting go of puzzles like this can be a slight weakness of mine) however I've still not fully worked it out. I had already found certain incompatibilities with the default DB settings and the SQL queries that I could find in the Go code. I assume a difference in defaults of the DB settings from the **pinned version** of the MySQL container used in the demo and current versions of MySQL as provided in the cloud explains why it works when using *only* containers for the demo. 

I got some of these issues fixed by researching the error codes returned when manually running these specific SQL queries from containers like in the above screenshot. However, the catalogue service never bit... Unfortunately the logging for all database connection errors present in the code was simply useless. All of the logging I could find in the source code logged the same generic message as seen below:

![](/blog/kpa_img/go_code.png)

I had a final thought to recompile the code with better error messaging to troubleshoot the connection issues further but when I hit up against compilation issues caused by the legacy packages used in the code (and ancient version of go), I knew it was finally time to give up. It was the devs' fault anyway, right? Not that I'm passing blame...

> If anyone does happen to know why it wasn't working though, please do let me know!


Apart from my own personal crusade at the end of the 8 weeks, we also did some socialising to remember that we are humans and not robots after such an intense educational journey. On the last day we all blew off some steam after delivering our presentations by attempting to break out of jail.


> I want to make this perfectly clear, I am not bitter about coming second in prison escape to one of the other Academy teams by a very narrow margin and am completely satisfied with my team's very high score of 1293! Completely.


![](/blog/kpa_img/group_photo.png) Left to right: John Boyle (Instructor), Jack Cullen, Damian Gillespie, Me (awkwardly holding the yellow coat), Josh Glass, Connor O'Kane, Owen Rooney, Rory Loughrey, Amilcar Garcia Triana.

We later were treated to a slap up meal in award winning Nu Deli - or so I believe. I unfortunately had to rush home early to attend to my wife, who had fallen ill, so I missed the feast. She will not ever be allowed to forget that she cost me free food!


### Summary

So after our first two months or so at Kainos, what had we learned.

For most of us, the following list was for the most part new:

- Terraform
- Docker
- Kubernetes
- Proper use of git
- Team coding
- Stand-ups and agile work methodologies
- CI/CD
- Macintosh personal computer machines

For some of us, AWS and Azure experience was also fairly limited. All in all, there was a lot of information to be absorbed by each of us and I think it's no exaggeration to say we probably learnt more in those two months that in most years in previous careers.

Coming from a diverse range of backgrounds, such as careers in IT support, network ops, mechanical engineering, some closer to University age than others and hailing from all over the province to Cuba, we all had valuable experiences to share with each other along the way. We learnt a lot from each other as much as from our instructor John and we've also gained a fantastic beginning network as we continue our careers within Kainos. The value in that I think is priceless and benefits us as well as Kainos. 

To think programs like this are so rare in the industry is truly tragic. Not only are we elevated by the fast track nature of the program, enabling us to up-skill and pivot into a new career area faster than we otherwise would have, Kainos gains truly diverse talent ensuring the capability keeps its skills pool broad and avoids falling into traps such as echo chambers which stifle innovation.

I am hugely grateful to Kainos, John, the guest speakers and my Academy colleagues for the experiences along the way and am eager to see what the future holds as I progress on to my first project.

Other organisations could learn a lot from Kainos about nurturing talent by providing programs such as this, not to mention their continued support and encouragement re continued training and growth.





[^1]: I should mention my personal view on TF is mixed, I see a lot of love for it in Kainos but I find myself wishing it was somehow *better* at times. I'd no prior experience of it specifically and I guess my expectations were too high based on non IaC languages such as Python. That's not to say it's bad per say, it's a relatively new area of code and clearly TF is better than many alternatives due to its seeming dominance, though I'd be interested in further exploring other options. I'm also very interested to see where OpenTofu goes, that whole shebang happened midway through the Academy. With the Linux Foundation backing it, it has the opportunity to be the new standard of IaC and perhaps one day it will be even more feature rich than TF. Who knows?

[^2]: A note on the combined issues running the docker tutorials. The main issue here was that the image wouldn't build on our hardware. This however was not caused by one factor alone but by a myriad of problems. First, the Apple silicone meant that some dependencies for the container were not available as binaries and needed locally compiled. Cross compiling for x86 seemed a viable option but many of us seemed to have had issues enabling QEMU due to admin permissions and the like. Lastly, at least one of the dependencies required a Python script to run in order to download it - it was an NPM package but it used Python in the build step for some reason (GYP or something... I'm not a JS guy). Python packages such as requests tend to use a downloaded copy of Mozilla's CA cert store and not make use of System certificates for SSL validation. Even if it did use system certs, the Python inside docker surely wouldn't. This meant that the Python installation inside the docker image being created didn't trust our ZScaler MITM inspection certificate and refused to download the NPM package. We ultimately came up with our solutions to simply run on a VM in the cloud to perform the lesson tasks but I later put some thought into the issue and wrote a quick Python script to install the ZScaler and Kainos CA certs into the certify location within my Python installation(s). A little bit of a hacky job but a useful one to know for future reference if ever playing with Python within the Kainos network. I do wonder if there are more standard solutions used by devs in the company? In theory I could have run this as a step in the DOCKERFILE but I never had time to test that out. I spent a little more time going down a rabbit hole trying to come up with a generic version of the script to patch any given Python installation/environment to trust any given MITM proxy certs (if matching an expected name - you wouldn't want to run that in a cafe!) without needing the certificates hardcoded within the script; but ultimately this does not seem to be possible due to how the proxy strips certain pieces of information from the HTTP requests. None-the-less, it was an interesting rabbit hole to navigate and I may write a future article on it specifically.